{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Submitting and scaling your first PySpark program\n",
    "\n",
    "This chapter covers\n",
    "\n",
    "- Summarizing data using `groupby` and a simple aggregate function\n",
    "- Ordering results for display\n",
    "- Writing data from a data frame\n",
    "- Using `spark-submit` to launch your program in batch mode\n",
    "- Simplifying PySpark writing using method chaining\n",
    "- Scaling your program to multiple files at once\n",
    "\n",
    "\n",
    "Chapter 2 dealt with all the data preparation work for our word frequency program. We read the input data, tokenized each word, and cleaned our records to only keep lowercase words. If we bring out our outline, we only have steps 4 and 5 to complete:\n",
    "\n",
    "- [DONE]Read: Read the input data (we’re assuming a plain text file).\n",
    "- [DONE]Token: Tokenize each word.\n",
    "- [DONE]Clean: Remove any punctuation and/or tokens that aren’t words. Lowercase each word.\n",
    "- Count: Count the frequency of each word present in the text.\n",
    "- Answer: Return the top 10 (or 20, 50, 100).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Grouping records: Counting word frequencies\n",
    "\n",
    "If you take our data frame in the same shape as it was at the end of chapter 2 (you can find the code in a single file in the book’s code repository at code/Ch02/end_of_ chapter.py), there is just a little more work to be done. With a data frame containing a single word per record, we just have to count the word occurrences and take the top contenders. This section shows you how to count records using the `GroupedData` object and perform an aggregation function—here, counting the items—on each group.\n",
    "\n",
    "Intuitively, we count the number of each word by creating groups: one for each word. Once those groups are formed, we can perform an **aggregation function** on each one of them. In this specific case, we count the number of records for each group, which will give us the number of occurrences for each word in the data frame. Under the hood, PySpark represents a grouped data frame in a `GroupedData` object; think of it as a transitional object that awaits an aggregation function to become a transformed data frame.\n",
    "\n",
    "![](https://drek4537l1klr.cloudfront.net/rioux/Figures/03-01.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# end-of-chapter.py############################################################\n",
    "#\n",
    "# Use this to get a free pass from Chapter 2 to Chapter 3.\n",
    "#\n",
    "# Remember, with great power comes great responsibility. Make sure you\n",
    "# understand the code before running it! If necessary, refer to the text in\n",
    "# Chapter 2.\n",
    "#\n",
    "###############################################################################\n",
    "\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import col, split, explode, lower, regexp_extract\n",
    "\n",
    "spark = SparkSession.builder.getOrCreate()\n",
    "\n",
    "book = spark.read.text(\"../../data/gutenberg_books/1342-0.txt\")\n",
    "\n",
    "lines = book.select(split(book.value, \" \").alias(\"line\"))\n",
    "\n",
    "words = lines.select(explode(col(\"line\")).alias(\"word\"))\n",
    "\n",
    "words_lower = words.select(lower(col(\"word\")).alias(\"word_lower\"))\n",
    "\n",
    "words_clean = words_lower.select(\n",
    "    regexp_extract(col(\"word_lower\"), \"[a-z]*\", 0).alias(\"word\")\n",
    ")\n",
    "\n",
    "words_nonull = words_clean.filter(col(\"word\") != \"\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The easiest way to count record occurrence is to use the `groupby()` method, passing the columns we wish to group as a parameter. The `groupby()`  method in listing 3.1 returns a `GroupedData` and awaits further instructions. Once we apply the `count()` method, we get back a data frame containing the grouping column word, as well as the count column containing the number of occurrences for each word."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<pyspark.sql.group.GroupedData at 0x222545b9e50>"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "groups = words_nonull.groupby(col(\"word\"))\n",
    "\n",
    "groups"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[word: string, count: bigint]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results = words_nonull.groupby(col(\"word\")).count()\n",
    " \n",
    "results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------+-----+\n",
      "|         word|count|\n",
      "+-------------+-----+\n",
      "|       online|    4|\n",
      "|         some|  203|\n",
      "|        still|   72|\n",
      "|          few|   72|\n",
      "|         hope|  122|\n",
      "|        those|   60|\n",
      "|     cautious|    4|\n",
      "|    imitation|    1|\n",
      "|          art|    3|\n",
      "|      solaced|    1|\n",
      "|       poetry|    2|\n",
      "|    arguments|    5|\n",
      "| premeditated|    1|\n",
      "|      elevate|    1|\n",
      "|       doubts|    2|\n",
      "|    destitute|    1|\n",
      "|    solemnity|    5|\n",
      "|   lieutenant|    1|\n",
      "|gratification|    1|\n",
      "|    connected|   14|\n",
      "+-------------+-----+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "results.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Peeking at the results data frame in listing 3.1, we see that the results are in no specific order. As a matter of fact, I’d be very surprised if you had the exact same order of words that I do! This has to do with how PySpark manages data. In chapter 1, we learned that PySpark distributes the data across multiple nodes. When performing a grouping function, such as `groupby()`, each worker performs the work on its assigned data. `groupby()` and `count()` are transformations, so PySpark will queue them lazily until we request an action. When we pass the `show()` method to our results data frame, it triggers the chain of computation that we see in figure 3.2.\n",
    "\n",
    "![](https://drek4537l1klr.cloudfront.net/rioux/Figures/03-02.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 3.1\n",
    "\n",
    "Starting with the `word_nonull` seen in this section, which of the following expressions would return the number of words per letter count (e.g., there are X one-letter words, Y two-letter words, etc.)?\n",
    "\n",
    "Assume that `pyspark.sql.functions.col`, `pyspark.sql.functions.length` are imported.\n",
    "\n",
    "a) `words_nonull.select(length(col(\"word\"))).groupby(\"length\").count()`\n",
    "\n",
    "b) `words_nonull.select(length(col(\"word\")).alias(\"length\")).groupby(\"length\").count()`\n",
    "\n",
    "c) `words_nonull.groupby(\"length\").select(\"length\").count()`\n",
    "\n",
    "d) None of those options would work.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[length: int, count: bigint]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from pyspark.sql.functions import length, col\n",
    "\n",
    "words_nonull.select(length(col(\"word\")).alias(\"length\")).groupby(\"length\").count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ordering the results on the screen using orderBy\n",
    "\n",
    "In 3.1, we explained why PySpark doesn’t necessarily maintain an order of records when performing transformations. If we look at our five-step blueprint, the last step is to return the top N records for different values of N. We already know how to show a specific number of records, so this section focuses on ordering the records in a data frame before displaying them:\n",
    "\n",
    "- [DONE]Read: Read the input data (we’re assuming a plain text file).\n",
    "- [DONE]Token: Tokenize each word.\n",
    "- [DONE]Clean: Remove any punctuation and/or tokens that aren’t words. Lowercase each word.\n",
    "- [DONE]Count: Count the frequency of each word present in the text.\n",
    "- Answer: Return the top 10 (or 20, 50, 100).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Just like we use `groupby()` to group a data frame by the values in one or many columns, we use `orderBy()` to order a data frame by the values of one or many columns. PySpark provides two different syntaxes to order records:\n",
    "\n",
    "- We can provide the column names as parameters, with an optional `ascending` parameter. By default, we order a data frame in ascending order; by setting `ascending` to `False`, we reverse the order, getting the largest values first.\n",
    "- Or we can use the Column object directly, via the `col` function. When we want to reverse the ordering, we use the `desc()` method on the column.\n",
    "\n",
    "PySpark orders the data frame using each column, one at a time. If you pass multiple columns (see chapter 5), PySpark uses the first column’s values to order the data frame, then the second (and then third, etc.) when there are identical values. Since we have a single column—and no duplicates because of `groupby()`—the application of `orderBy()` in the next listing is simple, regardless of the syntax we pick."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+-----+\n",
      "|word|count|\n",
      "+----+-----+\n",
      "| the| 4480|\n",
      "|  to| 4218|\n",
      "|  of| 3711|\n",
      "| and| 3504|\n",
      "| her| 2199|\n",
      "|   a| 1982|\n",
      "|  in| 1909|\n",
      "| was| 1838|\n",
      "|   i| 1750|\n",
      "| she| 1668|\n",
      "+----+-----+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "results.orderBy(\"count\", ascending=False).show(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+-----+\n",
      "|word|count|\n",
      "+----+-----+\n",
      "| the| 4480|\n",
      "|  to| 4218|\n",
      "|  of| 3711|\n",
      "| and| 3504|\n",
      "| her| 2199|\n",
      "|   a| 1982|\n",
      "|  in| 1909|\n",
      "| was| 1838|\n",
      "|   i| 1750|\n",
      "| she| 1668|\n",
      "+----+-----+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "results.orderBy(col(\"count\").desc()).show(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The list is very unsurprising: even though we can’t argue with Austen’s vocabulary, she isn’t immune to the fact that the English language needs pronouns and other common words. In natural language processing, those words are called stop words and could be removed. We solved our original query and can rest easy. Should you want to get the top 20, top 50, or even top 1,000, it’s easily done by changing the parameter to `show()`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> PySpark’s method naming convention zoo\n",
    ">\n",
    ">If you are detail-oriented, you might have noticed we used ``groupby`` (lowercase), but ``orderBy`` (lowerCamelCase, where you capitalize the first letter of each word but the first word). This seems like an odd design choice.\n",
    ">\n",
    ">``groupby()`` is an alias for ``groupBy()``, just like ``where()`` is an alias of ``filter()``. I guess that the PySpark developers found that a lot of typing mistakes were avoided by accepting the two cases. orderBy() didn’t have that luxury, for a reason that escapes my understanding, so we need to be mindful of this.\n",
    ">\n",
    ">Part of this incoherence is due to Spark’s heritage. Scala prefers camelCase for methods. On the other hand, we saw ``regexp_extract``, which uses Python’s preferred ``snake_case`` (words separated by an underscore) in chapter 2. There is no magic secret here: you’ll have to be mindful of the different case conventions at play in PySpark.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 3.2\n",
    "\n",
    "Why isn’t the order preserved in the following code block?\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------+-----+\n",
      "|length(word)|count|\n",
      "+------------+-----+\n",
      "|          12|  197|\n",
      "|           1|   10|\n",
      "|          13|  109|\n",
      "|           6|  897|\n",
      "|          16|    4|\n",
      "+------------+-----+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "(\n",
    "    results.orderBy(\"count\", ascending=False)\n",
    "    .groupby(length(col(\"word\")))\n",
    "    .count()\n",
    "    .show(5)\n",
    ")\n",
    "# +------------+-----+\n",
    "# |length(word)|count|\n",
    "# +------------+-----+\n",
    "# |          12|  199|\n",
    "# |           1|   10|\n",
    "# |          13|  113|\n",
    "# |           6|  908|\n",
    "# |          16|    4|\n",
    "# +------------+-----+\n",
    "# only showing top 5 rows\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Showing results on the screen is great for a quick assessment, but most of the time you’ll want them to have some sort of longevity. It’s much better to save those results to a file so that we’ll be able to reuse them without having to compute everything each time. The next section covers writing a data frame to a file."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Writing data from a data frame\n",
    "\n",
    "Just like we use ``read()`` and the ``SparkReader`` to read data in Spark, we use ``write()`` and the ``SparkWriter`` object to write back our data frame to disk. In listing 3.3, I specialize the ``SparkWriter`` to export text into a CSV file, naming the output ``simple_count.csv``. If we look at the results, we can see that PySpark didn’t create a ``results.csv`` file. Instead, it created a directory of the same name, and put 201 files inside the directory (200 CSVs + 1 _SUCCESS file)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Datentr�ger in Laufwerk D: ist Data\n",
      " Volumeseriennummer: D824-FAAC\n",
      "\n",
      " Verzeichnis von d:\\git\\manning\\DataAnalysisWithPythonAndPySpark\\data\\simple_count.csv\n",
      "\n",
      "28.04.2022  15:01    <DIR>          .\n",
      "28.04.2022  15:01    <DIR>          ..\n",
      "28.04.2022  15:01               604 .part-00000-dd9d26e2-484d-4ec0-b810-2aa10a13bf4c-c000.csv.crc\n",
      "28.04.2022  15:01                 8 ._SUCCESS.crc\n",
      "28.04.2022  15:01            76.075 part-00000-dd9d26e2-484d-4ec0-b810-2aa10a13bf4c-c000.csv\n",
      "28.04.2022  15:01                 0 _SUCCESS\n",
      "               4 Datei(en),         76.687 Bytes\n",
      "               2 Verzeichnis(se), 950.455.422.976 Bytes frei\n"
     ]
    }
   ],
   "source": [
    "results.write.csv(\"../../data/simple_count.csv\", mode=\"overwrite\")\n",
    " \n",
    "# The ls command is run using a shell, not a Python prompt.\n",
    "# If you use IPython, you can use the bang pattern (! ls -1).\n",
    "# Use this to get the same results without leaving the IPython console.\n",
    " \n",
    "#! ls -1 ../../data/simple_count.csv # for UNIX and MAC\n",
    "! dir ..\\..\\data\\simple_count.csv\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results.repartition(100).write.csv(\"../../data/simple_count.csv\", mode=\"overwrite\")\n",
    "\n",
    "#! ls -1 ../../data/simple_count.csv # for UNIX and MAC\n",
    "! dir ..\\..\\data\\simple_count.csv"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There it is, folks! The first moment where we have to care about PySpark’s distributed nature. Just like PySpark will distribute the transformation work across multiple workers, it’ll do the same for writing data. While it might look like a nuisance for our simple program, it is tremendously useful when working in distributed environments. When you have a large cluster of nodes, having many smaller files makes it easy to logically distribute reading and writing the data, making it way faster than having a single massive file.\n",
    "\n",
    "By default, PySpark will give you one **file per partition**. This means that our program, as run on my machine, yields 200 partitions at the end. This isn’t the best for portability. To reduce the number of partitions, we apply the ``coalesce()`` method with the desired number of partitions. The next listing shows the difference when using ``coalesce(1)`` on our data frame before writing to disk. We still get a directory, but there is a single CSV file inside of it. Mission accomplished!\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Datentr�ger in Laufwerk D: ist Data\n",
      " Volumeseriennummer: D824-FAAC\n",
      "\n",
      " Verzeichnis von d:\\git\\manning\\DataAnalysisWithPythonAndPySpark\\data\\simple_count_single_partition.csv\n",
      "\n",
      "28.04.2022  15:32    <DIR>          .\n",
      "28.04.2022  15:32    <DIR>          ..\n",
      "28.04.2022  15:32               604 .part-00000-0382faee-a0fa-4798-97c0-0ec26bba0cf5-c000.csv.crc\n",
      "28.04.2022  15:32                 8 ._SUCCESS.crc\n",
      "28.04.2022  15:32            76.075 part-00000-0382faee-a0fa-4798-97c0-0ec26bba0cf5-c000.csv\n",
      "28.04.2022  15:32                 0 _SUCCESS\n",
      "               4 Datei(en),         76.687 Bytes\n",
      "               2 Verzeichnis(se), 950.454.566.912 Bytes frei\n"
     ]
    }
   ],
   "source": [
    "results.coalesce(1).write.csv(\"../../data/simple_count_single_partition.csv\", mode=\"overwrite\")\n",
    " \n",
    "#! ls -1 ./data/simple_count_single_partition.csv/\n",
    "! dir ..\\..\\data\\simple_count_single_partition.csv"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> You might have realized that we’re not ordering the file before writing it. Since our data here is pretty small, we could have written the words by decreasing order of frequency. If you have a large data set, this operation will be quite expensive. Furthermore, since reading is a potentially distributed operation, what guarantees that it’ll get read the same way? Never assume that your data frame will keep the same ordering of records unless you explicitly ask via ``orderBy()`` right before the showing step."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Putting it all together: Counting\n",
    "\n",
    "The REPL allows you to go back in history using the directional arrows on your keyboard, just like a regular Python REPL. To make things a bit easier, I am providing the step-by-step program in the next listing. This section is dedicated to streamlining and making our code more succinct and readable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+-----+\n",
      "|word|count|\n",
      "+----+-----+\n",
      "| the| 4480|\n",
      "|  to| 4218|\n",
      "|  of| 3711|\n",
      "| and| 3504|\n",
      "| her| 2199|\n",
      "|   a| 1982|\n",
      "|  in| 1909|\n",
      "| was| 1838|\n",
      "|   i| 1749|\n",
      "| she| 1668|\n",
      "+----+-----+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import (\n",
    "    col,\n",
    "    explode,\n",
    "    lower,\n",
    "    regexp_extract,\n",
    "    split,\n",
    ")\n",
    " \n",
    "spark = SparkSession.builder.appName(\n",
    "    \"Analyzing the vocabulary of Pride and Prejudice.\"\n",
    ").getOrCreate()\n",
    " \n",
    "book = spark.read.text(\"../../data/gutenberg_books/1342-0.txt\")\n",
    " \n",
    "lines = book.select(split(book.value, \" \").alias(\"line\"))\n",
    " \n",
    "words = lines.select(explode(col(\"line\")).alias(\"word\"))\n",
    " \n",
    "words_lower = words.select(lower(col(\"word\")).alias(\"word\"))\n",
    " \n",
    "words_clean = words_lower.select(\n",
    "    regexp_extract(col(\"word\"), \"[a-z']*\", 0).alias(\"word\")\n",
    ")\n",
    " \n",
    "words_nonull = words_clean.where(col(\"word\") != \"\")\n",
    " \n",
    "results = words_nonull.groupby(col(\"word\")).count()\n",
    " \n",
    "results.orderBy(\"count\", ascending=False).show(10)\n",
    " \n",
    "results.coalesce(1).write.csv(\"../../data/simple_count_single_partition.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This program runs perfectly if you paste its entirety into the ``pyspark`` shell. With everything in the same file, we can make our code more friendly and make it easier for future you to come back to it. First, we adopt common import conventions when working with PySpark. We then rearrange our code to make it more readable, as seen in chapter 1."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Simplifying your dependencies with PySpark’s import conventions\n",
    "\n",
    "This program uses five distinct functions from the ``pyspark.sql.functions`` modules. We should probably replace this with a qualified import, which is Python’s way of importing a module by assigning a keyword to it. While there is no hard rule, the common wisdom is to use ``F`` to refer to PySpark’s functions. The next listing shows the before and after."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Before\n",
    "from pyspark.sql.functions import col, explode, lower, regexp_extract, split\n",
    " \n",
    "# After\n",
    "import pyspark.sql.functions as F"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since ``col``, ``explode``, ``lower``, ``regexp_extract``, and ``split`` are all in ``pyspark.sql.functions``, we can import the whole module. Since the new import statement imports the entirety of the ``pyspark.sql.functions`` module, we assign the keyword (or key letter) ``F``. The PySpark community seems to have implicitly settled on using ``F`` for ``pyspark.sql.functions``, and I encourage you to do the same. It’ll make your programs consistent, and since many functions in the module share their name with pandas or Python built-in functions, you’ll avoid name clashes. Each function application in the program will then be prefixed by ``F``, just like with regular Python-qualified imports."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Simplifying our program via method chaining\n",
    "\n",
    "If we look at the transformation methods we applied to our data frames (``select()``, ``where()``, ``groupBy()``, and ``count()``), they all have something in common: they take a structure as a parameter—the data frame or ``GroupedData`` in the case of ``count()``—and return a structure. All transformations can be seen as **pipes** that ingest a structure and return a modified structure. This section will look at method chaining and how it makes a program less verbose and thus easier to read by eliminating intermediate variables.\n",
    "\n",
    "In PySpark, every transformation returns an object, which is why we need to assign a variable to the result. This means that PySpark doesn’t perform modifications in place. For instance, the following code block by itself in a program wouldn’t do anything because we don’t assign the result to a variable.\n",
    "\n",
    "We can avoid intermediate variables by chaining the results of one method to the next. Since each transformation returns a data frame (or ``GroupedData``, when we perform the ``groupby()`` method), we can directly append the next method without assigning the result to a variable. This means that we can eschew all but one variable assignment. The code in the next listing shows the before and after. Note that we also added the ``F`` prefix to our functions to respect the import convention we outlined in section 3.4.1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Before\n",
    "book = spark.read.text(\"./data/gutenberg_books/1342-0.txt\")\n",
    " \n",
    "lines = book.select(split(book.value, \" \").alias(\"line\"))\n",
    " \n",
    "words = lines.select(explode(col(\"line\")).alias(\"word\"))\n",
    " \n",
    "words_lower = words.select(lower(col(\"word\")).alias(\"word\"))\n",
    " \n",
    "words_clean = words_lower.select(\n",
    "    regexp_extract(col(\"word\"), \"[a-z']*\", 0).alias(\"word\")\n",
    ")\n",
    " \n",
    "words_nonull = words_clean.where(col(\"word\") != \"\")\n",
    " \n",
    "results = words_nonull.groupby(\"word\").count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# After\n",
    "import pyspark.sql.functions as F\n",
    "\n",
    "results = (\n",
    "    spark.read.text(\"../../data/gutenberg_books/1342-0.txt\")\n",
    "    .select(F.split(F.col(\"value\"), \" \").alias(\"line\"))\n",
    "    .select(F.explode(F.col(\"line\")).alias(\"word\"))\n",
    "    .select(F.lower(F.col(\"word\")).alias(\"word\"))\n",
    "    .select(F.regexp_extract(F.col(\"word\"), \"[a-z']*\", 0).alias(\"word\"))\n",
    "    .where(F.col(\"word\") != \"\")\n",
    "    .groupby(\"word\")\n",
    "    .count()\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](https://drek4537l1klr.cloudfront.net/rioux/Figures/03-03.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results = spark\\\n",
    "          .read.text('../../data/ch02/1342-0.txt')\\"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Using spark-submit to launch your program in batch mode\n",
    "\n",
    "If we start PySpark with the ``pyspark`` program, the launcher takes care of creating the ``SparkSession`` for us. In chapter 2, we started from a basic Python REPL, so we created our entry point and named it spark. This section takes our program and submits it in batch mode. It is the equivalent of running a Python script; if you only need the result and not the REPL, this will do the trick.\n",
    "\n",
    "Unlike the interactive REPL, where the choice of language triggers the program to run, as in listing 3.10, we see that Spark provides a single program, named ``spark-submit``, to submit Spark (Scala, Java, SQL), PySpark (Python), and SparkR (R) programs. The full code for our program is available on the book’s repository under ``code/Ch02/word_count_submit.py``.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Usage: spark-submit [options] <app jar | python file | R file> [app arguments]\n",
      "Usage: spark-submit --kill [submission ID] --master [spark://...]\n",
      "Usage: spark-submit --status [submission ID] --master [spark://...]\n",
      "Usage: spark-submit run-example [options] example-class [example args]\n",
      "\n",
      "Options:\n",
      "  --master MASTER_URL         spark://host:port, mesos://host:port, yarn,\n",
      "                              k8s://https://host:port, or local (Default: local[*]).\n",
      "  --deploy-mode DEPLOY_MODE   Whether to launch the driver program locally (\"client\") or\n",
      "                              on one of the worker machines inside the cluster (\"cluster\")\n",
      "                              (Default: client).\n",
      "  --class CLASS_NAME          Your application's main class (for Java / Scala apps).\n",
      "  --name NAME                 A name of your application.\n",
      "  --jars JARS                 Comma-separated list of jars to include on the driver\n",
      "                              and executor classpaths.\n",
      "  --packages                  Comma-separated list of maven coordinates of jars to include\n",
      "                              on the driver and executor classpaths. Will search the local\n",
      "                              maven repo, then maven central and any additional remote\n",
      "                              repositories given by --repositories. The format for the\n",
      "                              coordinates should be groupId:artifactId:version.\n",
      "  --exclude-packages          Comma-separated list of groupId:artifactId, to exclude while\n",
      "                              resolving the dependencies provided in --packages to avoid\n",
      "                              dependency conflicts.\n",
      "  --repositories              Comma-separated list of additional remote repositories to\n",
      "                              search for the maven coordinates given with --packages.\n",
      "  --py-files PY_FILES         Comma-separated list of .zip, .egg, or .py files to place\n",
      "                              on the PYTHONPATH for Python apps.\n",
      "  --files FILES               Comma-separated list of files to be placed in the working\n",
      "                              directory of each executor. File paths of these files\n",
      "                              in executors can be accessed via SparkFiles.get(fileName).\n",
      "  --archives ARCHIVES         Comma-separated list of archives to be extracted into the\n",
      "                              working directory of each executor.\n",
      "\n",
      "  --conf, -c PROP=VALUE       Arbitrary Spark configuration property.\n",
      "  --properties-file FILE      Path to a file from which to load extra properties. If not\n",
      "                              specified, this will look for conf/spark-defaults.conf.\n",
      "\n",
      "  --driver-memory MEM         Memory for driver (e.g. 1000M, 2G) (Default: 1024M).\n",
      "  --driver-java-options       Extra Java options to pass to the driver.\n",
      "  --driver-library-path       Extra library path entries to pass to the driver.\n",
      "  --driver-class-path         Extra class path entries to pass to the driver. Note that\n",
      "                              jars added with --jars are automatically included in the\n",
      "                              classpath.\n",
      "\n",
      "  --executor-memory MEM       Memory per executor (e.g. 1000M, 2G) (Default: 1G).\n",
      "\n",
      "  --proxy-user NAME           User to impersonate when submitting the application.\n",
      "                              This argument does not work with --principal / --keytab.\n",
      "\n",
      "  --help, -h                  Show this help message and exit.\n",
      "  --verbose, -v               Print additional debug output.\n",
      "  --version,                  Print the version of current Spark.\n",
      "\n",
      " Cluster deploy mode only:\n",
      "  --driver-cores NUM          Number of cores used by the driver, only in cluster mode\n",
      "                              (Default: 1).\n",
      "\n",
      " Spark standalone or Mesos with cluster deploy mode only:\n",
      "  --supervise                 If given, restarts the driver on failure.\n",
      "\n",
      " Spark standalone, Mesos or K8s with cluster deploy mode only:\n",
      "  --kill SUBMISSION_ID        If given, kills the driver specified.\n",
      "  --status SUBMISSION_ID      If given, requests the status of the driver specified.\n",
      "\n",
      " Spark standalone, Mesos and Kubernetes only:\n",
      "  --total-executor-cores NUM  Total cores for all executors.\n",
      "\n",
      " Spark standalone, YARN and Kubernetes only:\n",
      "  --executor-cores NUM        Number of cores used by each executor. (Default: 1 in\n",
      "                              YARN and K8S modes, or all available cores on the worker\n",
      "                              in standalone mode).\n",
      "\n",
      " Spark on YARN and Kubernetes only:\n",
      "  --num-executors NUM         Number of executors to launch (Default: 2).\n",
      "                              If dynamic allocation is enabled, the initial number of\n",
      "                              executors will be at least NUM.\n",
      "  --principal PRINCIPAL       Principal to be used to login to KDC.\n",
      "  --keytab KEYTAB             The full path to the file that contains the keytab for the\n",
      "                              principal specified above.\n",
      "\n",
      " Spark on YARN only:\n",
      "  --queue QUEUE_NAME          The YARN queue to submit to (Default: \"default\").\n",
      "      \n"
     ]
    }
   ],
   "source": [
    "! spark-submit --help"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+-----+\n",
      "|word|count|\n",
      "+----+-----+\n",
      "| the| 4480|\n",
      "|  to| 4218|\n",
      "|  of| 3711|\n",
      "| and| 3504|\n",
      "| her| 2199|\n",
      "|   a| 1982|\n",
      "|  in| 1909|\n",
      "| was| 1838|\n",
      "|   i| 1749|\n",
      "| she| 1668|\n",
      "+----+-----+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: An illegal reflective access operation has occurred\n",
      "WARNING: Illegal reflective access by org.apache.spark.unsafe.Platform (file:/C:/Users/micha/Spark/spark-3.2.1-bin-hadoop3.2/jars/spark-unsafe_2.12-3.2.1.jar) to constructor java.nio.DirectByteBuffer(long,int)\n",
      "WARNING: Please consider reporting this to the maintainers of org.apache.spark.unsafe.Platform\n",
      "WARNING: Use --illegal-access=warn to enable warnings of further illegal reflective access operations\n",
      "WARNING: All illegal access operations will be denied in a future release\n",
      "Using Spark's default log4j profile: org/apache/spark/log4j-defaults.properties\n",
      "22/04/29 15:39:02 INFO SparkContext: Running Spark version 3.2.1\n",
      "22/04/29 15:39:02 INFO ResourceUtils: ==============================================================\n",
      "22/04/29 15:39:02 INFO ResourceUtils: No custom resources configured for spark.driver.\n",
      "22/04/29 15:39:02 INFO ResourceUtils: ==============================================================\n",
      "22/04/29 15:39:02 INFO SparkContext: Submitted application: Counting word occurences from a book.\n",
      "22/04/29 15:39:02 INFO ResourceProfile: Default ResourceProfile created, executor resources: Map(cores -> name: cores, amount: 1, script: , vendor: , memory -> name: memory, amount: 1024, script: , vendor: , offHeap -> name: offHeap, amount: 0, script: , vendor: ), task resources: Map(cpus -> name: cpus, amount: 1.0)\n",
      "22/04/29 15:39:02 INFO ResourceProfile: Limiting resource is cpu\n",
      "22/04/29 15:39:02 INFO ResourceProfileManager: Added ResourceProfile id: 0\n",
      "22/04/29 15:39:02 INFO SecurityManager: Changing view acls to: micha\n",
      "22/04/29 15:39:02 INFO SecurityManager: Changing modify acls to: micha\n",
      "22/04/29 15:39:02 INFO SecurityManager: Changing view acls groups to: \n",
      "22/04/29 15:39:02 INFO SecurityManager: Changing modify acls groups to: \n",
      "22/04/29 15:39:02 INFO SecurityManager: SecurityManager: authentication disabled; ui acls disabled; users  with view permissions: Set(micha); groups with view permissions: Set(); users  with modify permissions: Set(micha); groups with modify permissions: Set()\n",
      "22/04/29 15:39:02 INFO Utils: Successfully started service 'sparkDriver' on port 62484.\n",
      "22/04/29 15:39:02 INFO SparkEnv: Registering MapOutputTracker\n",
      "22/04/29 15:39:02 INFO SparkEnv: Registering BlockManagerMaster\n",
      "22/04/29 15:39:02 INFO BlockManagerMasterEndpoint: Using org.apache.spark.storage.DefaultTopologyMapper for getting topology information\n",
      "22/04/29 15:39:02 INFO BlockManagerMasterEndpoint: BlockManagerMasterEndpoint up\n",
      "22/04/29 15:39:02 INFO SparkEnv: Registering BlockManagerMasterHeartbeat\n",
      "22/04/29 15:39:02 INFO DiskBlockManager: Created local directory at C:\\Users\\micha\\AppData\\Local\\Temp\\blockmgr-6eb07bef-f9ef-44d7-b5cc-2352a37be104\n",
      "22/04/29 15:39:02 INFO MemoryStore: MemoryStore started with capacity 434.4 MiB\n",
      "22/04/29 15:39:02 INFO SparkEnv: Registering OutputCommitCoordinator\n",
      "22/04/29 15:39:03 WARN Utils: Service 'SparkUI' could not bind on port 4040. Attempting port 4041.\n",
      "22/04/29 15:39:03 INFO Utils: Successfully started service 'SparkUI' on port 4041.\n",
      "22/04/29 15:39:03 INFO SparkUI: Bound SparkUI to 0.0.0.0, and started at http://LAPTOP-PF4R1ML7:4041\n",
      "22/04/29 15:39:03 INFO Executor: Starting executor ID driver on host LAPTOP-PF4R1ML7\n",
      "22/04/29 15:39:03 INFO Utils: Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 62536.\n",
      "22/04/29 15:39:03 INFO NettyBlockTransferService: Server created on LAPTOP-PF4R1ML7:62536\n",
      "22/04/29 15:39:03 INFO BlockManager: Using org.apache.spark.storage.RandomBlockReplicationPolicy for block replication policy\n",
      "22/04/29 15:39:03 INFO BlockManagerMaster: Registering BlockManager BlockManagerId(driver, LAPTOP-PF4R1ML7, 62536, None)\n",
      "22/04/29 15:39:03 INFO BlockManagerMasterEndpoint: Registering block manager LAPTOP-PF4R1ML7:62536 with 434.4 MiB RAM, BlockManagerId(driver, LAPTOP-PF4R1ML7, 62536, None)\n",
      "22/04/29 15:39:03 INFO BlockManagerMaster: Registered BlockManager BlockManagerId(driver, LAPTOP-PF4R1ML7, 62536, None)\n",
      "22/04/29 15:39:03 INFO BlockManager: Initialized BlockManager: BlockManagerId(driver, LAPTOP-PF4R1ML7, 62536, None)\n",
      "22/04/29 15:39:03 INFO SharedState: Setting hive.metastore.warehouse.dir ('null') to the value of spark.sql.warehouse.dir.\n",
      "22/04/29 15:39:03 INFO SharedState: Warehouse path is 'file:/D:/git/manning/DataAnalysisWithPythonAndPySpark/code/Ch03/spark-warehouse'.\n"
     ]
    }
   ],
   "source": [
    "! spark-submit ./word_count_submit.py\n",
    " \n",
    "# [...]\n",
    "# +----+-----+\n",
    "# |word|count|\n",
    "# +----+-----+\n",
    "# | the| 4480|\n",
    "# |  to| 4218|\n",
    "# |  of| 3711|\n",
    "# | and| 3504|\n",
    "# | her| 2199|\n",
    "# |   a| 1982|\n",
    "# |  in| 1909|\n",
    "# | was| 1838|\n",
    "# |   i| 1749|\n",
    "# | she| 1668|\n",
    "# +----+-----+\n",
    "# only showing top 10 rows\n",
    "# [...]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> If you get a deluge of ``INFO`` messages, don’t forget that you have control over this: use ``spark.sparkContext.setLogLevel(\"WARN\")`` right after your spark definition. If your local configuration has ``INFO`` as a default, you’ll still get a slew of messages until it catches this line, but it won’t obscure your results."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Scaling up our word frequency program\n",
    "\n",
    "Teaching big data processing has a catch-22. While I want to show the power of PySpark to work with massive data sets, I don’t want you to purchase a cluster or rack up a massive cloud bill. It’s easier to show you the ropes using a smaller set of data, knowing that we can scale using the same code.\n",
    "\n",
    "Let’s take our word-counting example: How can we scale this to a larger corpus of text? Let’s download more files from Project Gutenberg and place them in the same directory:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Datentr�ger in Laufwerk D: ist Data\n",
      " Volumeseriennummer: D824-FAAC\n",
      "\n",
      " Verzeichnis von d:\\git\\manning\\DataAnalysisWithPythonAndPySpark\\data\\gutenberg_books\n",
      "\n",
      "26.04.2022  20:16    <DIR>          .\n",
      "29.04.2022  15:32    <DIR>          ..\n",
      "26.04.2022  20:16           173.595 11-0.txt\n",
      "26.04.2022  20:16           724.726 1342-0.txt\n",
      "26.04.2022  20:16           607.788 1661-0.txt\n",
      "26.04.2022  20:16         1.276.201 2701-0.txt\n",
      "26.04.2022  20:16         1.076.254 30254-0.txt\n",
      "26.04.2022  20:16           450.783 84-0.txt\n",
      "               6 Datei(en),      4.309.347 Bytes\n",
      "               2 Verzeichnis(se), 950.447.849.472 Bytes frei\n"
     ]
    }
   ],
   "source": [
    "! dir \"../../data/gutenberg_books\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We modify our ``word_count_submit.py`` in a very subtle way. Where we ``.read.text()``, we’ll change the path to account for all files in the directory. The next listing shows the before and after: we are only changing the ``1342-0.txt`` to a ``*.txt``, which is called a glob pattern. The ``*`` means that Spark selects all the ``.txt`` files in the directory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Count single book:  13427\n",
      "Count all books:  77910\n"
     ]
    }
   ],
   "source": [
    "# Before\n",
    "results = spark.read.text('../../data/gutenberg_books/1342-0.txt')\n",
    "print(\"Count single book: \", results.count())\n",
    " \n",
    "# After\n",
    "results = spark.read.text('../../data/gutenberg_books/*.txt')\n",
    "print(\"Count all books: \", results.count())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> You can also just pass the name of the directory if you want PySpark to ingest all the files within the directory."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With this, you can confidently say that you can scale a simple data analysis program using PySpark. You can use the general formula we’ve outlined here and modify some of the parameters and methods to fit your use case. Chapters 4 and 5 will dig a little deeper into some interesting and common data transformations, building on what we’ve learned here.\n",
    "\n",
    "## Summary\n",
    "\n",
    "- You can group records using the ``groupby`` method, passing the column names you want to group against as a parameter. This returns a ``GroupedData`` object that waits for an aggregation method to return the results of computation over the groups, such as the ``count()`` of records.\n",
    "- PySpark’s repertoire of functions that operates on columns is located in ``pyspark.sql.functions``. The unofficial but well-respected convention is to qualify this import in your program using the ``F`` keyword.\n",
    "- When writing a data frame to a file, PySpark will create a directory and put one file per partition. If you want to write a single file, use the ``coaslesce(1)`` method.\n",
    "- To prepare your program to work in batch mode via ``spark-submit``, you need to create a ``SparkSession``. PySpark provides a builder pattern in the ``pyspark.sql`` module.\n",
    "- If your program needs to scale across multiple files within the same directory, you can use a glob pattern to select many files at once. PySpark will collect them in a single data frame.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Additional Exercises\n",
    "\n",
    "For these exercises, you’ll need the ``word_count_submit.py`` program we worked on in this chapter. You can pick it from the book’s code repository (``Code/Ch03/word_ count_submit.py``)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "### Exercise 3.3\n",
    "\n",
    "1. By modifying the ``word_count_submit.py`` program, return the number of distinct words in Jane Austen’s Pride and Prejudice. (Hint: results contains one record for each unique word.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "6595"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "import pyspark.sql.functions as F\n",
    "\n",
    "\n",
    "spark = SparkSession.builder.appName(\n",
    "    \"Counting word occurences from a book.\"\n",
    ").getOrCreate()\n",
    "\n",
    "spark.sparkContext.setLogLevel(\"WARN\")\n",
    "\n",
    "# If you need to read multiple text files, replace `1342-0` by `*`.\n",
    "results = (\n",
    "    spark.read.text(\"../../data/gutenberg_books/1342-0.txt\")  # or \"*.txt\" for all txt files, or only the parent dir\n",
    "    .select(F.split(F.col(\"value\"), \" \").alias(\"line\"))\n",
    "    .select(F.explode(F.col(\"line\")).alias(\"word\"))\n",
    "    .select(F.lower(F.col(\"word\")).alias(\"word\"))\n",
    "    .select(F.regexp_extract(F.col(\"word\"), \"[a-z']*\", 0).alias(\"word\"))\n",
    "    .where(F.col(\"word\") != \"\")\n",
    "    .groupby(F.col(\"word\"))\n",
    "    .count()\n",
    ")\n",
    "\n",
    "results.count()\n",
    "#results.coalesce(1).write.csv(\"../../data/results_single_partition.csv\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "6595"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "import pyspark.sql.functions as F\n",
    "\n",
    "\n",
    "spark = SparkSession.builder.appName(\n",
    "    \"Counting word occurences from a book.\"\n",
    ").getOrCreate()\n",
    "\n",
    "spark.sparkContext.setLogLevel(\"WARN\")\n",
    "\n",
    "# If you need to read multiple text files, replace `1342-0` by `*`.\n",
    "results = (\n",
    "    spark.read.text(\"../../data/gutenberg_books/1342-0.txt\")\n",
    "    .select(F.split(F.col(\"value\"), \" \").alias(\"line\"))\n",
    "    .select(F.explode(F.col(\"line\")).alias(\"word\"))\n",
    "    .select(F.lower(F.col(\"word\")).alias(\"word\"))\n",
    "    .select(F.regexp_extract(F.col(\"word\"), \"[a-z']*\", 0).alias(\"word\"))\n",
    "    .where(F.col(\"word\") != \"\")\n",
    "    .distinct()\n",
    ")\n",
    "\n",
    "results.count()\n",
    "#results.coalesce(1).write.csv(\"../../data/results_single_partition.csv\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. (Challenge) Wrap your program in a function that takes a file name as a parameter. It should return the number of distinct words."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "23754"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def get_unique_word_count(file: str):\n",
    "    results = (\n",
    "        spark.read.text(file)\n",
    "        .select(F.split(F.col(\"value\"), \" \").alias(\"line\"))\n",
    "        .select(F.explode(F.col(\"line\")).alias(\"word\"))\n",
    "        .select(F.lower(F.col(\"word\")).alias(\"word\"))\n",
    "        .select(F.regexp_extract(F.col(\"word\"), \"[a-z']*\", 0).alias(\"word\"))\n",
    "        .where(F.col(\"word\") != \"\")\n",
    "        .distinct()\n",
    "    )\n",
    "\n",
    "    return results.count()\n",
    "\n",
    "get_unique_word_count(\"../../data/gutenberg_books/*.txt\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 3.4\n",
    "\n",
    "Taking ``word_count_submit.py``, modify the script to return a sample of five words that appear only once in Jane Austen’s Pride and Prejudice.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------+-----+\n",
      "|        word|count|\n",
      "+------------+-----+\n",
      "|   imitation|    1|\n",
      "|     solaced|    1|\n",
      "|premeditated|    1|\n",
      "|     elevate|    1|\n",
      "|   destitute|    1|\n",
      "+------------+-----+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "import pyspark.sql.functions as F\n",
    "\n",
    "\n",
    "spark = SparkSession.builder.appName(\n",
    "    \"Counting word occurences from a book.\"\n",
    ").getOrCreate()\n",
    "\n",
    "spark.sparkContext.setLogLevel(\"WARN\")\n",
    "\n",
    "# If you need to read multiple text files, replace `1342-0` by `*`.\n",
    "results = (\n",
    "    spark.read.text(\"../../data/gutenberg_books/1342-0.txt\")\n",
    "    .select(F.split(F.col(\"value\"), \" \").alias(\"line\"))\n",
    "    .select(F.explode(F.col(\"line\")).alias(\"word\"))\n",
    "    .select(F.lower(F.col(\"word\")).alias(\"word\"))\n",
    "    .select(F.regexp_extract(F.col(\"word\"), \"[a-z']*\", 0).alias(\"word\"))\n",
    "    .where(F.col(\"word\") != \"\")\n",
    "    .groupBy(F.col(\"word\"))\n",
    "    .count()\n",
    "    .filter(F.col(\"count\") == 1)\n",
    ")\n",
    "\n",
    "results.show(5)\n",
    "#results.coalesce(1).write.csv(\"../../data/results_single_partition.csv\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 3.5\n",
    "\n",
    "1. Using the ``substring`` function (refer to PySpark’s API or the pyspark shell if needed), return the top five most popular first letters (keep only the first letter of each word).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------+-----+\n",
      "|first_letter|count|\n",
      "+------------+-----+\n",
      "|           t|16101|\n",
      "|           a|13684|\n",
      "|           h|10419|\n",
      "|           w| 9091|\n",
      "|           s| 8791|\n",
      "+------------+-----+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "import pyspark.sql.functions as F\n",
    "\n",
    "\n",
    "spark = SparkSession.builder.appName(\n",
    "    \"Counting word occurences from a book.\"\n",
    ").getOrCreate()\n",
    "\n",
    "spark.sparkContext.setLogLevel(\"WARN\")\n",
    "\n",
    "# If you need to read multiple text files, replace `1342-0` by `*`.\n",
    "results = (\n",
    "    spark.read.text(\"../../data/gutenberg_books/1342-0.txt\")\n",
    "    .select(F.split(F.col(\"value\"), \" \").alias(\"line\"))\n",
    "    .select(F.explode(F.col(\"line\")).alias(\"word\"))\n",
    "    .select(F.lower(F.col(\"word\")).alias(\"word\"))\n",
    "    .select(F.regexp_extract(F.col(\"word\"), \"[a-z']*\", 0).alias(\"word\"))\n",
    "    .where(F.col(\"word\") != \"\")\n",
    "    .select(F.substring(F.col(\"word\"), 1, 1).alias(\"first_letter\"))\n",
    "    .groupBy(F.col(\"first_letter\"))\n",
    "    .count()\n",
    ")\n",
    "\n",
    "results.orderBy(\"count\", ascending=False).show(5)\n",
    "#results.coalesce(1).write.csv(\"../../data/results_single_partition.csv\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. Compute the number of words starting with a consonant or a vowel. (Hint: The isin() function might be useful.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "33522"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "import pyspark.sql.functions as F\n",
    "\n",
    "\n",
    "spark = SparkSession.builder.appName(\n",
    "    \"Counting word occurences from a book.\"\n",
    ").getOrCreate()\n",
    "\n",
    "spark.sparkContext.setLogLevel(\"WARN\")\n",
    "\n",
    "# If you need to read multiple text files, replace `1342-0` by `*`.\n",
    "words_with_vowel = (\n",
    "    spark.read.text(\"../../data/gutenberg_books/1342-0.txt\")\n",
    "    .select(F.split(F.col(\"value\"), \" \").alias(\"line\"))\n",
    "    .select(F.explode(F.col(\"line\")).alias(\"word\"))\n",
    "    .select(F.lower(F.col(\"word\")).alias(\"word\"))\n",
    "    .select(F.regexp_extract(F.col(\"word\"), \"[a-z']*\", 0).alias(\"word\"))\n",
    "    .where(F.col(\"word\") != \"\")\n",
    "    .filter(F.substring(F.col(\"word\"), 1, 1).isin(\"a\",\"e\",\"i\",\"o\",\"u\").alias(\"starts_with_vowel\"))\n",
    "    .count()\n",
    ")\n",
    "\n",
    "words_with_vowel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "88653"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "words_without_vowel = (\n",
    "    spark.read.text(\"../../data/gutenberg_books/1342-0.txt\")\n",
    "    .select(F.split(F.col(\"value\"), \" \").alias(\"line\"))\n",
    "    .select(F.explode(F.col(\"line\")).alias(\"word\"))\n",
    "    .select(F.lower(F.col(\"word\")).alias(\"word\"))\n",
    "    .select(F.regexp_extract(F.col(\"word\"), \"[a-z']*\", 0).alias(\"word\"))\n",
    "    .where(F.col(\"word\") != \"\")\n",
    "    .filter(~F.substring(F.col(\"word\"), 1, 1).isin(\"a\",\"e\",\"i\",\"o\",\"u\").alias(\"starts_with_vowel\"))\n",
    "    .count()\n",
    ")\n",
    "\n",
    "words_without_vowel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "127368"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(\n",
    "    spark.read.text(\"../../data/gutenberg_books/1342-0.txt\")\n",
    "    .select(F.split(F.col(\"value\"), \" \").alias(\"line\"))\n",
    "    .select(F.explode(F.col(\"line\")).alias(\"word\"))\n",
    "    .select(F.lower(F.col(\"word\")).alias(\"word\"))\n",
    "    .count()\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 3.6\n",
    "\n",
    "Let’s say you want to get both the ``count()`` and ``sum()`` of a ``GroupedData`` object. Why doesn’t this code work? Map the inputs and outputs of each method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "my_data_frame.groupby(\"my_column\").count().sum()"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "ed3b52db287b4a5df29473a599c0c2e9f2908e1b4c36ea301cf500c1489a061a"
  },
  "kernelspec": {
   "display_name": "Python 3.9.12 ('pyspark')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
